# Week 2: Regression with Multiple Input Variables

## Module Summary
This week built upon the foundations of linear regression by extending it to multiple features. I learned the importance of vectorization for efficiency and feature scaling for faster convergence of gradient descent. The focus then shifted to classification problems, introducing logistic regression and the sigmoid function to predict probabilities. A key concept was the decision boundary, which visually separates the classes learned by the model.

## Key Concepts
*   **Multiple Linear Regression:** Extending linear regression to handle multiple input features (`x₁, x₂, ... xₙ`).
*   **Feature Scaling and Mean Normalization:** Rescaling input features to a similar range to speed up gradient descent convergence.
*   **Logistic Regression for Classification:** Using the sigmoid function to model the probability of a binary outcome (0 or 1).
*   **Decision Boundary:** The line or surface that separates different classes in the feature space.

## Resources
*   [Week 2 Notes](./week-2-notes.md)
