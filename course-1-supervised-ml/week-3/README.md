# Week 3: Classification & Overfitting

## Module Summary
This week focused on improving the performance and generalization of machine learning models. I learned to diagnose the problems of overfitting (high variance) and underfitting (high bias). The main technique to combat overfitting, regularization (both L1 and L2), was introduced, which adds a penalty to the cost function to keep model parameters small. Finally, I learned about the Normal Equation as a direct, non-iterative method to solve for the optimal parameters in linear regression, understanding its advantages and disadvantages compared to Gradient Descent.

## Key Concepts
*   **The Problem of Overfitting:** Understanding high bias (underfitting) vs. high variance (overfitting) and how they affect model generalization.
*   **Regularization:** Adding a penalty term (L1 or L2) to the cost function to prevent overfitting by discouraging complex models.
*   **The Normal Equation:** An analytical method to solve for linear regression parameters directly without iteration, suitable for smaller datasets.

## Resources
*   [Week 3 Notes](./week-3-notes.md)
