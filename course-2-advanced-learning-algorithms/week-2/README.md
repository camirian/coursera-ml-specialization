# Week 2: Training Neural Networks

## Module Summary
This week was all about how to train neural networks effectively. The core algorithm, backpropagation, was introduced as the method for calculating the gradients needed to update the network's weights. I learned how to verify my implementation of backpropagation using gradient checking. The important topic of hyperparameter tuning was also covered, along with more advanced optimization algorithms like Adam, which can significantly speed up the training process.

## Key Concepts
*   **Training Neural Networks & Backpropagation:** The algorithm for updating weights using gradient descent.
*   **Gradient Checking:** Verifying the correctness of backpropagation implementation.
*   **Hyperparameter Tuning:** Strategies for choosing the right parameters (learning rate, layers, etc.).
*   **Optimization Algorithms:** Advanced methods like Adam and Momentum for faster convergence.

## Resources
*   [Week 2 Notes](./week-2-notes.md)
