# Week 3: Decision Trees and Ensembles

## Module Summary
This week explored powerful non-linear models, moving beyond neural networks. I learned about decision trees, an intuitive and interpretable model, and how they are built using concepts like information gain. The main focus was on tree ensembles, which combine multiple decision trees to achieve high performance. I was introduced to Random Forests and Gradient Boosted Trees. Finally, the week covered practical advice for applying machine learning, such as the importance of splitting data into training, development, and test sets.

## Key Concepts
*   **Decision Trees:** Building trees using information gain and Gini impurity.
*   **Tree Ensembles:** Random Forests and Gradient Boosted Trees (XGBoost).
*   **Advice for Applying Machine Learning:** Train/Dev/Test sets and preventing overfitting.

## Resources
*   [Week 3 Notes](./week-3-notes.md)
